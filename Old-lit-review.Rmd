---
output:
  word_document: default
  pdf_document: default
---

```{r include_packages, include = FALSE}
library(knitr)
library(palmerpenguins)
library(tidyverse)
library(nycflights13)
data(flights)

library(ggpcp)
library(ggplot2)
library(dplyr)
data(nasa)

library(scales)
library(datasets)
data("ChickWeight")
library(formatR)

```

# General Introduction

Statisticians use graphs in almost every stage of their work. 
They create charts to summarize and explore new data and identify potential problems and opportunities. 
Models are fit based on relationships between variables which are often identified visually. 
We identify problems with those models based on residual plots and other visual diagnostics. 
When our modeling work has been completed, we present our results to interested parties using visual displays, because non-statisticians often find it easier to understand data and models through an intuitive visual medium rather than through the mathematical formulae which underlie the statistical work.

Given the wide range of uses for graphs and visual data displays in statistical modeling, it is unsurprising that some graphs are more useful for specific applications, such as exploratory analysis, and are unsuitable for other applications, such as presenting to an outside group. 
In addition, not all visual displays have equal perceptual value [@unwin2003, @aspillaga1996]. 
The best graphics are designed to account for both the dataset and the intended audience's features.
Some design constraints stem from limitations of the human perceptual system and are common to most potential consumers of the visualization. 
For example, the sine illusion affects anyone with binocular depth perception, and color recommendations are built around the specific characteristics of the human retina [@vanderplas2015].
Other design constraints are due to the audience’s experience level and if they are used to working with data and understand specialized techniques (e.g., enough familiarity with principal component analysis such that a plot of factor loadings might be useful). 
Do they understand specialized techniques such as principal component analysis to the point where a plot of factor loadings might be a useful visual display?
When we create visualizations for public consumption, we have to consider both perceptual factors and the target audience’s domain knowledge.
This introduction presents previous research on constructing interactive and static visual displays for different audiences and the implications of this research when designing interactive data displays such as dashboards.

Most research in statistical graphics has been done on static graphics; usually, research also strips away all but the most essential contextual information, sacrificing external validity for statistical control. 
As a result, it can be hard to generalize this research to practical applications, where the contextual information surrounding the data is critical and the chart does not just exist in a vacuum.

In the real world, however, conventions and familiarity often win out over best practice validated by perceptual experiments.
For example, in sports, many coaches desire printable diagrams containing all necessary and valuable information on a single page. 
As data in sports becomes more prominent, extensive, and collected, this information must be refined. 

Thus, in addition to the experimental evidence, we must consider the human element: how to introduce new graphical concepts to stakeholders, and the considerations involved in encouraging stakeholders to adopt these improved graphics.
Let us first consider the audience characteristics that affect the selection of graphics. 
Then, we will engage with considerations based on the data to be displayed. 
Finally, we will consider the interactions between the audience and the data: how graphics are tested, amended, and hopefully eventually adopted into common use.

## Methodology - Exploratory Data Analysis (EDA)

John Tukey was the first to organize the collection and methods associated with philosophy into Exploratory Data Analysis (EDA). 
Previous research by Tukey focused on graphics as a tool for exploratory analysis. 
In "Exploratory Data Analysis," Tukey wrote that graphics and charts often display data with more enhanced understanding than a table, [@tukey1966]. 
Tukey outlines detailed the types of different graphics and in which situations to utilize these graphics. 
He was a strong advocate for the importance of EDA as a crucial first step in the data analysis process and emphasized the need for visualization and interactive techniques to understand patterns and relationships in data.

Tukey's Principles of EDA have become a cornerstone in the field of statistics and have been adopted by data professionals in various industries.
Tukey's principles have enabled data professionals to understand complex data sets better and make more informed decisions by emphasizing the importance of visual exploration, data characterization, and model critique. 
In this way, Tukey's Principles have revolutionized our data analysis approach and become the foundational framework for EDA.

Tukey's Principles in EDA:

1. Graphical exploration, looking for patterns or displaying fit, the method demonstrates things about data that a single numeric metric does not understand. 
This has been useful in graphing the data before you develop summary statistics.

2. Describing the general patterns of the data. 
This step should be insensitive to outliers. 
In general, think about the types of resistant measures (i.e., median or mean). 
This step is making sure to determine data patterns.

3. The natural scale/state that the data are at their best. 
This will be the step at which the scale of data can be helpful for analysis. 
The reexpressing data to a new scale by taking the square root or logarithmic scale.

4. The mostly known parts of EDA but is done in the way of accessing fit of the data. 
This is taught in every statistics 101 class. 
The growth of machine learning and prediction methods have now used residuals more in the toolbox to assessing the best prediction models.

```{r violin_plot, echo=FALSE,out.width='.49\\linewidth', fig.width=3, fig.height=3, fig.show='asis', fig.align='center'}
ggplot(ChickWeight, aes(x = Diet, y = weight)) + 
  geom_violin(aes(fill = Diet), trim = FALSE) +
  geom_boxplot(width = 0.1) +
  theme_classic() +
  theme(legend.position = "none")

p <- ggplot(ChickWeight, aes(x=Diet, y=weight, fill=Diet)) +
  geom_boxplot()
p + scale_fill_brewer(palette="Dark2")
```

Data visualizations are an integral part of the EDA process, enabling analysts to discern patterns and relationships in the data that would otherwise be difficult to discern from tabular data alone.
Through data visualization, analysts can quickly identify trends, outliers, and other patterns that may be missed through numerical analysis alone. 
Moreover, visualizations facilitate the communication of findings to non-technical stakeholders, allowing them to comprehend complex data sets more efficiently. 
Through visualizations, analysts can also identify potential issues or biases in the data, resulting in better decisions and models. 
Thus, visualizations play a crucial role in the EDA process by enabling analysts to more effectively explore, comprehend, and communicate data-derived insights.
During the initial EDA stage, an analyst may find that a variable or a covariate is directly related to the dependent variable when looking at a correlation heatmap or a scatterplot. 
The basic understanding can be formalized to visualize the discovery process.

The field of graphical communication, which is directly related to EDA, semiology, and their use in touch, has been a valuable tool and extension of the EDA thoughts that Tukey expressed.
One of the fundamental principles of semiology is the relationship between signifier and signified, in which a visual element (the signifier) represents a particular meaning or concept (the signified), [@barthes1972]. 
Another essential concept in semiology is using syntax and semantics to convey meaning in graphic communication effectively. 
This includes both the syntax and semantics of a graphic's visual elements, [@bertin1983].

Using color to represent data on maps is an example of successful graphical communication utilizing semiology. 
By using different colors to represent different data points, viewers can comprehend patterns and relationships in the data quickly and easily. 
Jacques Bertin  writes in "Semiology of Graphics" that color can be used to "emphasize a point, distinguish one category from another, or establish a relationship between two points", [@bertin1983].
In addition, Bertin explains that the use of color can help overcome language barriers, making it easier for the audience to comprehend the presented information.

The application of semiology in graphical communication is not devoid of obstacles. 
One difficulty is the possibility of misinterpretation, in which viewers may assign a different meaning to a visual element than was intended, [@bertin1983]. 
Another concern is the possibility of cultural differences in interpretation, in which a visual element may have a different meaning in one culture versus another, [@norman2013].

Exploratory Data Analysis (EDA) analyzes and summarizes a dataset to discover patterns, trends, and insights. 
It is a crucial step in the data analysis process and is often used to identify which variables are essential, what the data looks like, and what the underlying structure of the data is. 
EDA is typically done using various techniques, such as visualizations, statistical summaries, and data transformations.


## Open-Source Data: Suitability and Integration

The concept of open source originated long before computers, where shared knowledge formed the basis of human progress.
Originating from the frustrations of figures like Richard Stallman in the 1980s, who championed the free software movement and paved the way for the creation of transformative licenses such as the GNU General Public License, the concept has grown to encompass more than just software. [@stallman2002]
With organizations like the Open Source Initiative standardizing open-source practices and monumental projects like Linux showcasing its potential, open-source principles expanded by the late 1990s to include data.
This movement towards openness has been marked by a commitment to transparency, collaboration, and unrestricted access, revolutionizing how we perceive and interact with data in the modern era.
Linux, an open-source operating system kernel initiated by Linus Torvalds in 1991, became one of the most popular examples of open-source success.
The Apache HTTP Server, released in 1995, became another success story, powering a large fraction of the internet [@weber2004].
Open-source principles began expanding from software to data in the late 1990s and early 2000s. The idea was to share data sets for public use without restrictions.
Projects like the Human Genome Project advocated for open data to advance science and medicine [@sulston2002].

The principles of open-source data revolve around the concepts of free access, transparency, collaboration, and redistribution, fostering a community-driven approach to data sharing and utilization.
At its core, open source is about making the source content (usually software code) freely available. 
This transparency allows anyone to review, inspect, and understand the source.
The freedom to use, modify, distribute, and study the source content without restrictions is a cornerstone of open source.
This is articulated in various open source licenses, like the GNU General Public License (GPL), which emphasizes the rights of end users.
Open source projects thrive on collaborative efforts. 
Diverse groups of people from around the world contribute, enhancing the project's robustness and creativity.
Tools like Git and platforms like GitHub have made this collaborative approach more streamlined.
Open source projects often foster strong communities. 
These communities not only contribute code but also offer support, documentation, and strategies for the project's future.
These communities' sense of belonging and shared purpose is a driving force behind many successful open-source projects.
Contributions to open source projects are often evaluated based on merit. 
The best ideas or implementations, regardless of their source, are adopted, promoting a culture of excellence.
The principle of redistribution ensures that modified versions of open source content remain open. 
This ensures a perpetual cycle of community-driven improvement and access.
Open source principles prioritize the end user’s interests. 
This user-centric approach often leads to software or content that's more aligned with what users genuinely need and want.


### Open Access Data Repositories

In the digital age, open access data repositories have become crucial platforms, transforming how data are shared, accessed, and stored among academic and research communities. 
These repositories are online spaces created specifically to store datasets and make them available to everyone. In line with the principles of open science, which promote knowledge sharing, transparency, and reproducibility in research endeavors, their main objective is to democratize access to data.

These repositories serve many different purposes at their core. 
They support openness in the scientific method first and foremost. 
It enables other researchers and the general public to examine, confirm, and replicate research findings by providing unrestricted access to datasets. Collaboration is also encouraged by this open model. 
Datasets are accessible to researchers in a variety of fields and locations, providing opportunities to expand upon, combine, or compare various sets of data. 
Eliminating research duplication is a significant additional benefit. 
Access to data from earlier studies reduces the need to repeatedly collect similar data, which saves time, effort, and resources. 
Additionally, these repositories are essential for the long-term preservation of data, guaranteeing that datasets are accessible for future scientific research and for posterity.

A closer look at the features of these repositories reveals several standard components. 
Metadata is paramount; this descriptive information elucidates the context, origin, methodology, and other pertinent details of the datasets, ensuring they are comprehensible to those accessing them. 
Many repositories also emphasize the citability of datasets. 
By assigning a Digital Object Identifier (DOI), datasets become easily referable in academic and research contexts. 
Licensing is another cornerstone. To clarify usage rights and conditions, datasets are often accompanied by explicit licenses, with Creative Commons licenses being particularly prevalent. 
These licenses can stipulate various conditions, with attribution being a common requirement. 
Furthermore, the user experience is enhanced through tools that facilitate easy searching, accessing, and downloading of datasets.

However, like all systems, open access data repositories face challenges. 
Ensuring data privacy is a significant concern, especially in datasets derived from human subjects, such as in health or social research. 
Standardization is another hurdle. 
Given the diverse range of researchers and datasets being uploaded, achieving a standardized format for data and metadata is daunting. 
And not to be overlooked is the challenge of sustainability. 
Maintaining a sophisticated digital platform requires both financial resources and technical expertise. 
Ensuring the longevity of such repositories, especially in a rapidly evolving digital landscape, remains a concern.

Several prominent examples underscore the significance and diversity of open access data repositories. 
Zenodo, developed under the European OpenAIRE program, serves as a multifaceted platform catering to researchers across disciplines. 
Dryad specializes in datasets linked to scientific publications, predominantly in life sciences and biomedicine. 
Figshare offers a broader spectrum, allowing researchers to deposit a range of research outputs, including datasets, making them available to the public.

Historically, the rise of these repositories can be contextualized within the broader open science movement. 
This movement, gathering momentum over recent decades, has been advocating for a more transparent, accessible, and shared research process. 
As technological advancements surged, leading to an exponential growth in digital data, the emphasis on preserving and sharing this data treasure trove intensified.

Today, the significance of open access data repositories is further underscored by policies and guidelines from influential bodies. 
A growing number of funding agencies and academic journals are either mandating or recommending that data underpinning research findings be housed in open access repositories. 
This trend not only ensures that research outputs, especially data, remain accessible but also reflects a broader societal push towards transparency, especially when research is funded by public coffers.


<!--     - Single Data Source with multiple variables (n > 10 columns) 
## Single Open-source Data

A single data source refers to a specific origin or repository from which data is derived or collected, ensuring uniformity and consistency in the information presented. 
It contrasts with multiple data sources where data is aggregated from various origins, potentially introducing discrepancies or inconsistencies. 
For example, a company might maintain a single database that logs all customer transactions. 
This database would be considered a single data source. 
Another illustration could be a researcher conducting a study on climate change, who decides to rely solely on temperature readings from a specific meteorological station; in this case, the meteorological station's data would be the single data source.
Utilizing a single data source simplifies data management and analysis but may also limit the available breadth and diversity of information. -->

<!--     - Multiple Data Sources with multiple variables (n > 10 columns) 

## Multiple Open-source Data

Multiple data sources refer to the amalgamation of data from various origins or repositories, providing a comprehensive and diverse piece of information. 
By drawing from different sources, analysts can gain a richer and more holistic view of the subject, although it may introduce data integration and consistency. 
For instance, in business intelligence, a company might combine sales data from their online store, physical retail outlets, and third-party distributors to get a complete picture of their sales performance. 
Similarly, a researcher studying urban air quality might merge satellite observations, ground-based air quality monitors, and citizen-reported data to achieve a multifaceted understanding. 
Leveraging multiple data sources offers depth and breadth in insights but necessitates careful management to ensure data integrity and comparability. -->


## R package Development - Past, Present and Future 
R has become a prominent and adaptable programming language within the domain of data analysis and exploratory data analysis (EDA). 
A number of R packages have been created to streamline the processes of data manipulation, visualization, and statistical analysis. 
One of the prominent programs in this category is `ggplot2`, which was developed by Hadley Wickham. 
This package is widely recognized for its versatility and aesthetic appeal in generating a diverse range of visually appealing and informative data visualizations [@wickham2016]. 
Wickham et al. have developed `dplyr` and `tidyr`, which offer convenient functionalities for filtering, summarizing, and reshaping data frames, hence enhancing the efficiency of data manipulation activities [@wickham2021]. 
The `data.table` package, developed by Matt Dowle, is widely recognized for its remarkable speed and efficiency in handling huge datasets, particularly in the context of performance-oriented data manipulation tasks [@dowle2021]. 
Although not classified as a R package, it is noteworthy to include Wes McKinney's Python library, `pandas`, due to its significant influence in the field of data manipulation and analysis in the Python programming language [@mcKinney2010]. 
Within the field of psychology and psychometrics, the `psych` package developed by William Revelle serves as an extensive tool for doing factor analysis and reliability analysis [@revelle202]. 

Furthermore, there are other software packages available that may be utilized by researchers and analysts to enhance their data management and analysis processes. 
For instance, the `naniar` package, developed by [@tierney2020], is specifically designed for handling missing data. Another useful package is `summarytools`, created by [@comtois2021], which enables the generation of descriptive statistics. 
Additionally, the `corrplot` package, developed by Wei and Simko [@wei2017], provides significant tools for displaying correlation matrices. 
These packages provide researchers and analysts a range of valuable resources to aid in their work. 
The `FactoMineR` package, developed by Husson, Lê, and Pagès [@husson2020], offers crucial techniques for conducting multivariate data analysis, including principal component analysis and clustering. 
These software programs, created by renowned individuals in the discipline, jointly provide data analysts and scientists with the necessary tools to efficiently investigate and analyze data.

    
## Dimension Reduction

Exploratory Data Analysis (EDA) and Dimension Reduction Techniques are closely intertwined in the realm of data analysis. 
EDA serves as the initial step in understanding and visualizing complex datasets, helping analysts uncover patterns, anomalies, and relationships within the data. 
However, as datasets grow in dimensionality, the complexity of EDA can become overwhelming. This is where Dimension Reduction Techniques come into play. 
By reducing the number of features or variables while preserving the most critical information, these methods simplify the data, making it more manageable for further analysis. 
Dimension Reduction Techniques, such as Principal Component Analysis (PCA) or t-Distributed Stochastic Neighbor Embedding (t-SNE), complement EDA by providing a way to condense high-dimensional data into a lower-dimensional space without losing significant insights. 
This synergy between EDA and Dimension Reduction Techniques empowers data scientists and analysts to gain deeper insights into their data while efficiently handling large and complex datasets.


## Interactive Graphics

Interactive graphics are essential to EDA [@unwin1999]. 
Beyond the limitations of static statistical displays, interactive graphics enable visualizations to advance alongside the analysis. 
User interaction and direct manipulation are required for dynamic graphics to reach their full potential (@cook1995; @unwin1999).
The connection between EDA and dashboards is that EDA is the process of preparing and understanding the data, which is the first step for building a dashboard, as the data has to be cleaned, transformed, and analyzed to be used efficiently on the dashboard. 
EDA results can be used to identify the most relevant data and metrics to include in the dashboard and to design the visualizations that will be used to display the data. 
Additionally, the EDA process can identify the outliers, patterns, trends, and insights helpful to show in the dashboard to support decision-making.
<!--     - The importance of the grammar of data manipulation (dplyr and tidyverse)  -->




<!-- - What is Data Manipulation? -->
<!--     - A formal definition and construction of data manipulation  -->
<!--     - The application of data manipulation  -->
<!-- - Summary -->



Through a multi-phase investigation, this dissertation will:

1. Review the current landscape of dashboard design principles and best practices.

2. Analyze the characteristics and quality of open-source data and its suitability for dashboard integration.

3. Conduct user-based experiments to determine the challenges and pitfalls naive users face when interacting with dashboards.

4. Propose design recommendations for dashboards that effectively convey statistical information to naive users, ensuring correct visual inference and minimizing misinterpretations.

The findings from this research aim to bridge the gap between data experts and naive users, democratizing data interpretation and empowering more individuals to make informed decisions based on open-source data.
