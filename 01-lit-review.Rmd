---
output:
  word_document: default
  pdf_document: default
---

```{r include_packages, include = FALSE, echo=}
library(knitr)
library(palmerpenguins)
library(tidyverse)
library(nycflights13)
data(flights)

library(ggpcp)
library(ggplot2)
library(dplyr)
data(nasa)

library(scales)
library(datasets)
data("ChickWeight")
library(formatR)

```

## Introduction

Our data-driven modern civilization now depends heavily on the ability to precisely evaluate and exploit huge and complex datasets, especially in small rural towns where resources are often sparse. 
This dissertation aims to make big datasets more understandable and useful by utilizing three related innovations: generalized parallel coordinate plots (GPCPs), reliable ways to look at missing data, and efficient methods to elaborate on the detection of unusual trends.
Each priority area is key for improving the GPCP technique to enable thorough and accurate data analysis across various datasets.

The accuracy of data interpretation is highest when dealing directly with numerical values or one-dimensional visual representations (Spence, 1990). 
That is becoming more and more impractical. The size and dimensionality of datasets are increasing. 
In order to make judgments based on data, it is necessary to find methods of simplifying and consolidating datasets without sacrificing significant information. 
Often, a data visualization, such as a graph or table, serves as a go-between step between raw data and an observer's decision-making process. A data visualization is the visual depiction of information. 
Essentially, it is an effort to effectively communicate synthesized data pieces in a way that is easy for an observer to understand and evaluate. 

For high-dimensional data, parallel coordinate plots (PCPs) are a common visualization tool whereby data points are shown as lines intersecting each axis, therefore indicating a dimension. 
Conventional PCP hinders its interpretability by having clutter and overlapping lines while handling big datasets. By including methods such as axis reordering, bundling, and dimensionality reduction to solve these problems, generalized parallel coordinate plots (GPCPs) expand the conventional PCP. 
These improvements in clarity and usability of PCPs by means of data pattern highlighting and visual clutter reduction help Heinrich and Weiskopf (2013), for example, present several approaches to bundle comparable trajectories in GPCPs, so minimizing overlap and improving pattern identification; Johannsen et al. (2012) address the advantages of dynamic axis reordering to highlight different data characteristics.

Stimulus-responsive attention is typically triggered by a certain category, where an observer learns that one element of the stimulus contains valuable information about the most promising location to search for additional information in order to make an accurate judgment. 
Greensmith's "Personal Equation of Interaction" investigates personal variations in cognitive and perceptual capacity when using sophisticated visual interfaces (Greensmith, 2016). 
This theory emphasizes the need of human characteristics in understanding and classifying composite glyphs, graphical elements expressing multidimensional data. 
Applied to generalized parallel coordinate plots (GPCPs), it underlines the requirement of considering individual cognitive and perceptual variances since these influence data interpretation accuracy and efficiency. Generalized parallel coordinate charts display many dimensions within a single glyph.

Overall, this dissertation addresses theoretical and practical gaps within the existing literature and offers tangible improvements to GPCP's functionality. 
By enhancing the treatment of missing data, refining outlier detection, and upgrading statistical methodologies, this research aims to extend GPCP's applicability across diverse research fields, ultimately facilitating more accurate and insightful data analysis. 
Through a mix of theoretical expansion, algorithm development, and practical application, the enhancements proposed herein seek to advance the field of data visualization significantly.

## Cognition Models and Data Visualization

Data visualization professionals are impacted by advancements in cognitive science, as evidenced by multiple research studies (Healey & Enns, 2011; Hegarty, 2011; Crapo, Waisel, Wallace & Willemain, 2000). Additionally, cognitive scientists contribute to developing ideas about data visualization (e.g., Kosslyn, 2006; Green, Ribarsky & Fisher, 2009; Pinker, 1990; Lewandowsky & Spence, 1989). 

However, data visualization models primarily clarify the connection between users and the display rather than describe the cognitive processes involved in response to data visualizations.
 
The following will present a concise overview of data visualization methods that assert the influence of the cognitive system. 

This is done partly to discover patterns and demonstrate the current level of abstraction in comprehending the connection between the observer and the data. Most models that establish a connection between the observer and data in data visualization do not. 

These experiments manipulate low-level visual features not explicitly incorporated into current data visualization models. Several of the theories below show the absence of a clear relationship between perception and cognition.

Data visualization models are generally abstract, process-oriented descriptions of how the user interacts with the image.
Waisal, Wallace & Willemain (1999) put forth a data visualization theory that proposes mental models as how observers comprehend the world. 
Mental models are cognitive constructs believed by some to serve as the foundation for relational reasoning, as proposed by Johnson-Laird in 1983 and further supported by Byrne and Johnson-Laird in 1989. Mental models have three essential elements: understanding, description, and validation. 
The theory of data visualization proposed by Waisal and colleagues corresponds to the initial stage of a mental model, where information is encoded, and the cognitive system readies itself for further elaboration. 
The model proposed by Waisal et al. describes a series of processes that an observer must follow to synchronize their mental representation of the information in the display with the actual display. 
The system takes the observer's understanding of the data as input and generates a response that can be verified. 
The initial step of the Waisal et al. model involves constructing a mental representation based on the observers' understanding of the data visualization. 
Subsequently, an observer selects a certain perspective from this mental model and accurately reproduces it based on the observed graph. 
To make progress, the observer needs to assess three things: 1) if the visualization accurately corresponds to the model view, 2) if the visualization is consistent with the mental model, and 3) if the view and visualization are feasible when taking external factors into account. 
Once all three conditions are satisfied, the observer proceeds to the validation stage of the mental model. 
If any criteria are not satisfied, the mental model is revised, and the view is derived based on the revised mental model.

<!-- INSERT IMAGE HERE -->


The practical implications of the research are evident in the data obtained from the think-aloud technique and mouse-tracking. 
These findings, as reported by Waisel et al. (1999), indicate that viewers do utilize certain steps in the process of comprehending the data visualization. 
While the sequence of stages was not easily identifiable from the observed data, the authors propose that suggesting separate steps was beneficial for isolating sub-processes and gaining a clearer grasp of the description aspect involved in constructing a mental model. 
This insight can be valuable in designing more effective data visualizations that align with the cognitive processes of the viewers.

Pinker's theory (1990) states that the mind is made of a collection of separate, natural mental modules that were developed to manage particular kinds of data and activities. 
This thoery offers a unique perspective on the cognitive components involved in understanding graphs at a higher level. 
It includes a matching process, which utilizes a graph schema of the suitable type, a message assembly that converts visual information into conceptual information, an interrogation stage where additional information is extracted from the graph if necessary, and finally an inferential process where decisions are made or insights are generated. 

<!-- INSERT IMAGE HERE -->

Each of these four procedures involves highly intricate matters. 
The conversion of visual input into a conceptually meaningful form is a complex task, yet it is encapsulated in a single step in this model. 
However, this is not necessarily a drawback. 
If we consider this theory as a means to effectively break down the challenge of data visualization into smaller, more manageable components, then it is beneficial to divide an overwhelmingly large problem (how individuals comprehend graphs) into more meaningful sub-problems, such as understanding how people transform visual data into meaningful information. Pinker defines the matching process as a component of "cognitive fit", which Vessey and his colleagues have separately investigated as a distinct problem.

For people to understand the data, how the information is presented is important. Traditionally, scatterplots are great for showing how two variables are related, bar graphs are great for showing a key property of multiple levels of a single variable, and histograms are great for showing univariate distributions. Cognitive fit is accomplished when the designer creates the appropriate tool (graph) for the task and the observer comprehends the correlation between their objective and the tool (Vessey & Galletta, 1991). Designing a visualization for cognitive fit involves creating compatible visual representations that accurately capture the mental representation of data. 

Furthermore, if we want to show that a value is higher for one observation than another, we can visually display this difference on the graph's Y-axis (Bertin, 1983). 

In his groundbreaking work "Semiology of Graphics" (1983), Jacques Bertin laid out a theory that changed the way we think about and use graphs to show information. Bertin said that images are not just pictures; they are important tools for communicating and analyzing data. Bertin came up with the idea of "retinal variables" – visual attributes such as position, size, color, orientation, shape, and texture – that can be changed to store and read data. Bertin states that good visualizations should use these visual attributes to create patterns that are hard to see from text or numbers alone.


<!-- INSERT IMAGE HERE -->

Bertin's theory is important in the areas of cartography and statistical graphics. Categorical, numeric, and quantitative data were the three main types of data he classified. It was important for him to emphasize how important it was to choose the right visual attributes based on the data and the message of the graphic.


Attaining optimal cognitive fit minimizes the cognitive load on the observer when carrying out the current task. The cognitive fit concept is grounded in the theory of information processing (Newell & Simon, 1972) and the recognition that individuals balance error and effort when carrying out tasks (Beach & Mitchell, 1978; Payne, 1982; Vessey, 1994; Wickelgren, 1977). Cognitive fit is one of several models that are based on an information processing framework. Pinker identifies as the matching process is encompassed by “cognitive fit”, explored by Vessey and colleagues as its own discrete problem.

Models of data visualization that make explicit the generation of the visualization (the data to the graphic stage) and the perception of the data (the graphic to the viewer) are helpful in thinking about a scientific framework for data visualization. There are two high-level cognitive viewpoints at play: the creator and the viewer. Wijk’s model (2005) is an example of how the connection between components can be nicely explained in an information-processing manner. The system (comprised of the data and the viewer) changes as a function of operations applied between data (input) and graphics (output). Knowledge is updated in response to information extracted from the image, where the amount of information depends on the cognitive and perceptual properties of the observer. van Wijk’s model captures initial parts of cognitive psychology as one of three stages for environmental information extraction.


The simple model skips over a lot of interesting details, but it does so in a useful way. To see the big picture, you have to zoom out and look at the big picture links between data and people. Otherwise, you won't be able to make real progress toward its parts. Taking into account both the person and the data as a whole system pushes theory to accurately show how the two parts relate to each other. 

<!-- INSERT IMAGE HERE -->

Both the force-directed edge bundling (FDEB) technique and the Knowledge-Assisted Visualization and Guidance (KAVA) model enlarge the visualization capacity, as suggested by van Wijk. By combining explicit knowledge bases and automated analytical tools, the KAVA paradigm improves visual analytics and enables more accurate and informed representations across many applications. Conversely, FDEB bundles edges using a self-organizing technique that represents edges as flexible springs, reducing visual clutter in node-link diagrams and revealing high-level patterns without needing hierarchical structures. Van Wijk's simplified approach was less suited to efficiently handle complicated, high-dimensional data since it mostly concentrated on basic data display features without such advanced integrations.

<!-- INSERT IMAGE HERE -->

It's helpful to think about information processors hierarchically to understand the links between data, visualization, people, and choices. Wijk's model is very high-level and talks about the whole system. On the other hand, a lower-level information processing model can describe the mental steps a user needs to take to understand a data visualization. In their model, Simkin and Hastie (1987) say that basic processes (similar to cognitive fit) happen between what the viewer sees and how well they understand the job. While Pinker develops this into a more complex theory, considering extra cognitive processes and proposing a disciplined model for graph comprehension, Simkin and Hastie offer the first exploration of the cognitive phases involved. Pinker's work is thus an extension and elaboration of the fundamental ideas presented by Simkin and Hastie, so offering a better knowledge of how graphical information is handled and interpreted by users. These basic processes are recognition, scanning, projection, and superimposition. Another thing that is important for making sense of a graph is grounding.

Researchers have found that visual anchoring changes how people see a stimulus when they are comparing (Johnson & Pashler, 1990) or mentally rotating things (Xu & Franconeri, 2015). Anchoring is a process that is only talked about in theories of data visualization, but it is also a key part of other kinds of cognitive tasks (Couclelis, Gollegdge, Gale & Tobler, 1987; Pylyshyn, 1999), which shows that it is at least a basic part of how people think. This kind of process is very interesting because it helps make sense of data visualizations and learning more about it can improve the way we think in general. 

A more standard model of information processing with input, memory, and output (Neisser, 1967) is changed to fit the task of making a choice based on a data visualization (Patterson, Blaha, Georges, Grinstein, Liggett, Kaveney, Sheldon, Havig, & Moore, 2014). The authors were smart to include a process that captures attention and clarifies output. It is the first form of data visualization that lets bottom-up processes change how people understand graphs independently. In their model, the stimulus serves as the input, with encoding being the first step. Once encoded, the information can be modified in both long-term and working memory. Working memory and pattern recognition can alter long-term memory and influence the storage process. Together, working memory and pattern recognition collaborate to make decisions, guiding subsequent reactions.

<!-- INSERT IMAGE HERE -->

It's helpful to think about information processors hierarchically to understand the links between data, visualization, people, and choices. Wijk's model is very high-level and talks about the whole system. On the other hand, a lower-level information processing model can describe the mental steps a user needs to take to understand a data visualization. In their model, Simkin and Hastie (1987) say that basic processes (similar to cognitive fit) happen between what the viewer sees and how well they understand the job. Simkin and Hastie present the initial investigation of the cognitive phases involved while Pinker extends this into a more complicated theory considering extra cognitive processes and suggests a disciplined model for graph understanding. Pinker's work thus serves as an extension and elaboration of Simkin and Hastie's basic principles, so providing a greater grasp of how graphical information is processed and perceived by users. These fundamental procedures are recognition, scanning, projection, and superimposition. Grounding is still another crucial component for understanding a graph. This model shows the main groups of processes that need to be known to create a science of data visualization. Still, it doesn't go into detail about the different parts that are important in making the stimulus (in this case, the data visualization).

In the models we've already talked about, visual information comes in the form of a graph or plot, which is then stored in memory to consolidate into long-term memory. In previous basic cognition research, changes to the visual world were studied independently to see how they affected simpler tasks. Key models and theories as discussed earlier in Pinker's cognitive model, which distinguishes top-down and bottom-up processes in visual comprehension and stresses the integration of mental representations with current knowledge, build the foundations of basic vision research in cognitive psychology (Pinker, 1990; Hegarty, 2011). Introduced by Wertheimer, gestalt ideas draw attention to systems of perceptual organization including figure-ground separation (Spillmann, 2012). Furthermore underlined by information theory post-WWII, research on attention mechanisms highlights the part of signal detection and cognitive control in visual perception (Swets, 1964; Posner, 2004). By merging theoretical models with actual research, these essential books together progress our knowledge of visual cognition. New research in how we understand data visualization builds on the strong base that basic vision research has laid. 

Many things in the world need to be processed carefully, but this study shows that some visual details can be processed without paying full attention. "Pre-attentive" processing is the faster, more automatic processing of the visual environment. Research shows that it takes less than 200 milliseconds (< 200ms) of preattentive processing to allow quick, parallel detection of core visual characteristics including color, orientation, and size without deliberate attention. Introduced by Treisman and Gelade in 1980, Feature Integration Theory (FIT) provides that while individual features are handled preattentively, integrating them into an overall object requires focused attention, so explaining why conjunction searches—identifying goals based on multiple features—are slower and indicate for a serial search approach. This approach emphasizes the part attention plays in binding several preattentive elements into a single perceptual experience (Mokeichev, Segev, & Ben-Shahar, 2010; Treisman & Gelade, 1980; Wolfe, 2021). In contrast "attentive" processing is the slower, more conscious processing of a part of the visual environment. So, making visualizations that support pre-attentive processing should free up the viewer's brain resources so they can be used more effectively in figuring out what the data means (Healey, Booth & Enns, 1995), which will lead to a better way of sharing data.

In a survey conducted of basic research in attention, Healey and Enns (2011) identified clusters of tasks that are supported by pre-attentive processing: target detection; finding a goal-relevant unique item in an array, boundary detection; identifying distinct spatially organized, texture-based groups based upon how they differ from each other, region tracking;  multiple object tracking and counting. These findings clearly provide an evidence-based manner for making design decisions when generating data visualizations. Suppose the goal of the creator is to show the difference between groups on two different variables, for example. In that case, the creator may use a set of features, like color hues or blending, that allows a viewer to pre-attentively discern boundaries between the two groups on a scatter plot (Callaghan, 1990).





## Exploratory Data Analysis (EDA)

John Tukey was the first to organize the collection and methods associated with philosophy into Exploratory Data Analysis (EDA). 
Previous research by Tukey focused on graphics as a tool for exploratory analysis. 
In "Exploratory Data Analysis," Tukey wrote that graphics and charts often display data with more enhanced understanding than a table, [@tukey1966]. 
Tukey outlines detailed the types of different graphics and in which situations to utilize these graphics. 
He was a strong advocate for the importance of EDA as a crucial first step in the data analysis process and emphasized the need for visualization and interactive techniques to understand patterns and relationships in data.

Tukey's Principles of EDA have become a cornerstone in the field of statistics and have been adopted by data professionals in various industries.
Tukey's principles have enabled data professionals to understand complex data sets better and make more informed decisions by emphasizing the importance of visual exploration, data characterization, and model critique. 
In this way, Tukey's Principles have revolutionized our data analysis approach and become the foundational framework for EDA.

Tukey's Principles in EDA:

1. Graphical exploration, looking for patterns or displaying fit, the method demonstrates things about data that a single numeric metric does not understand. 
This has been useful in graphing the data before you develop summary statistics.

2. Describing the general patterns of the data. 
This step should be insensitive to outliers. 
In general, think about the types of resistant measures (i.e., median or mean). 
This step is making sure to determine data patterns.

3. The natural scale/state that the data are at their best. 
This will be the step at which the scale of data can be helpful for analysis. 
The reexpressing data to a new scale by taking the square root or logarithmic scale.

4. The mostly known parts of EDA but is done in the way of accessing fit of the data. 
This is taught in every statistics 101 class. 
The growth of machine learning and prediction methods have now used residuals more in the toolbox to assessing the best prediction models.

```{r violin_plot, echo=FALSE,out.width='.49\\linewidth', fig.width=3, fig.height=3, fig.show='asis', fig.align='center'}
ggplot(ChickWeight, aes(x = Diet, y = weight)) + 
  geom_violin(aes(fill = Diet), trim = FALSE) +
  geom_boxplot(width = 0.1) +
  theme_classic() +
  theme(legend.position = "none")

p <- ggplot(ChickWeight, aes(x=Diet, y=weight, fill=Diet)) +
  geom_boxplot()
p + scale_fill_brewer(palette="Dark2")
```

Data visualizations are an integral part of the EDA process, enabling analysts to discern patterns and relationships in the data that would otherwise be difficult to discern from tabular data alone.
Through data visualization, analysts can quickly identify trends, outliers, and other patterns that may be missed through numerical analysis alone. 
Moreover, visualizations facilitate the communication of findings to non-technical stakeholders, allowing them to comprehend complex data sets more efficiently. 
Through visualizations, analysts can also identify potential issues or biases in the data, resulting in better decisions and models. 
Thus, visualizations play a crucial role in the EDA process by enabling analysts to more effectively explore, comprehend, and communicate data-derived insights.
During the initial EDA stage, an analyst may find that a variable or a covariate is directly related to the dependent variable when looking at a correlation heatmap or a scatterplot. 
The basic understanding can be formalized to visualize the discovery process.

The field of graphical communication, which is directly related to EDA, semiology, and their use in touch, has been a valuable tool and extension of the EDA thoughts that Tukey expressed.
One of the fundamental principles of semiology is the relationship between signifier and signified, in which a visual element (the signifier) represents a particular meaning or concept (the signified), [@barthes1972]. 
Another essential concept in semiology is using syntax and semantics to convey meaning in graphic communication effectively. 
This includes both the syntax and semantics of a graphic's visual elements, [@bertin1983].

Using color to represent data on maps is an example of successful graphical communication utilizing semiology. 
By using different colors to represent different data points, viewers can comprehend patterns and relationships in the data quickly and easily. 
Jacques Bertin  writes in "Semiology of Graphics" that color can be used to "emphasize a point, distinguish one category from another, or establish a relationship between two points", [@bertin1983].
In addition, Bertin explains that the use of color can help overcome language barriers, making it easier for the audience to comprehend the presented information.

The application of semiology in graphical communication is not devoid of obstacles. 
One difficulty is the possibility of misinterpretation, in which viewers may assign a different meaning to a visual element than was intended, [@bertin1983]. 
Another concern is the possibility of cultural differences in interpretation, in which a visual element may have a different meaning in one culture versus another, [@norman2013].

Exploratory Data Analysis (EDA) analyzes and summarizes a dataset to discover patterns, trends, and insights. 
It is a crucial step in the data analysis process and is often used to identify which variables are essential, what the data looks like, and what the underlying structure of the data is. 
EDA is typically done using various techniques, such as visualizations, statistical summaries, and data transformations.

## R package Development - Past, Present and Future 
R has become a prominent and adaptable programming language within the domain of data analysis and exploratory data analysis (EDA). 
A number of R packages have been created to streamline the processes of data manipulation, visualization, and statistical analysis. 
One of the prominent programs in this category is `ggplot2`, which was developed by Hadley Wickham. 
This package is widely recognized for its versatility and aesthetic appeal in generating a diverse range of visually appealing and informative data visualizations [@wickham2016]. 
Wickham et al. have developed `dplyr` and `tidyr`, which offer convenient functionalities for filtering, summarizing, and reshaping data frames, hence enhancing the efficiency of data manipulation activities [@wickham2021]. 
The `data.table` package, developed by Matt Dowle, is widely recognized for its remarkable speed and efficiency in handling huge datasets, particularly in the context of performance-oriented data manipulation tasks [@dowle2021]. 
Although not classified as a R package, it is noteworthy to include Wes McKinney's Python library, `pandas`, due to its significant influence in the field of data manipulation and analysis in the Python programming language [@mcKinney2010]. 
Within the field of psychology and psychometrics, the `psych` package developed by William Revelle serves as an extensive tool for doing factor analysis and reliability analysis [@revelle202]. 

Furthermore, there are other software packages available that may be utilized by researchers and analysts to enhance their data management and analysis processes. 
For instance, the `naniar` package, developed by [@tierney2020], is specifically designed for handling missing data. Another useful package is `summarytools`, created by [@comtois2021], which enables the generation of descriptive statistics. 
Additionally, the `corrplot` package, developed by Wei and Simko [@wei2017], provides significant tools for displaying correlation matrices. 
These packages provide researchers and analysts a range of valuable resources to aid in their work. 
The `FactoMineR` package, developed by Husson, Lê, and Pagès [@husson2020], offers crucial techniques for conducting multivariate data analysis, including principal component analysis and clustering. 
These software programs, created by renowned individuals in the discipline, jointly provide data analysts and scientists with the necessary tools to efficiently investigate and analyze data.

    
## Dimension Reduction

Exploratory Data Analysis (EDA) and Dimension Reduction Techniques are closely intertwined in the realm of data analysis. 
EDA serves as the initial step in understanding and visualizing complex datasets, helping analysts uncover patterns, anomalies, and relationships within the data. 
However, as datasets grow in dimensionality, the complexity of EDA can become overwhelming. This is where Dimension Reduction Techniques come into play. 
By reducing the number of features or variables while preserving the most critical information, these methods simplify the data, making it more manageable for further analysis. 
Dimension Reduction Techniques, such as Principal Component Analysis (PCA) or t-Distributed Stochastic Neighbor Embedding (t-SNE), complement EDA by providing a way to condense high-dimensional data into a lower-dimensional space without losing significant insights. 
This synergy between EDA and Dimension Reduction Techniques empowers data scientists and analysts to gain deeper insights into their data while efficiently handling large and complex datasets.


## Interactive Graphics

Interactive graphics are essential to EDA [@unwin1999]. 
Beyond the limitations of static statistical displays, interactive graphics enable visualizations to advance alongside the analysis. 
User interaction and direct manipulation are required for dynamic graphics to reach their full potential (@cook1995; @unwin1999).
The connection between EDA and dashboards is that EDA is the process of preparing and understanding the data, which is the first step for building a dashboard, as the data has to be cleaned, transformed, and analyzed to be used efficiently on the dashboard. 
EDA results can be used to identify the most relevant data and metrics to include in the dashboard and to design the visualizations that will be used to display the data. 
Additionally, the EDA process can identify the outliers, patterns, trends, and insights helpful to show in the dashboard to support decision-making.
