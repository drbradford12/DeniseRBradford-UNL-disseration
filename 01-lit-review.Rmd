# General Introduction

## Exploratory Data Analysis (EDA)

John Tukey was the first to organize the collection and methods associated with philosophy into Exploratory Data Analysis (EDA). 
Previous research by Tukey focused on graphics as a tool for exploratory analysis. 
In "Exploratory Data Analysis," Tukey wrote that graphics and charts often display data with more enhanced understanding than a table, [@tukey1966]. 
Tukey outlines detailed the types of different graphics and in which situations to utilize these graphics. 
He was a strong advocate for the importance of EDA as a crucial first step in the data analysis process and emphasized the need for visualization and interactive techniques to understand patterns and relationships in data.

Tukey's Principles of EDA have become a cornerstone in the field of statistics and have been adopted by data professionals in various industries.
Tukey's principles have enabled data professionals to understand complex data sets better and make more informed decisions by emphasizing the importance of visual exploration, data characterization, and model critique. 
In this way, Tukey's Principles have revolutionized our data analysis approach and become the foundational framework for EDA.

Tukey's Principles in EDA:

1. Graphical exploration, looking for patterns or displaying fit, the method demonstrates things about data that a single numeric metric does not understand. 
This has been useful in graphing the data before you develop summary statistics.

2. Describing the general patterns of the data. 
This step should be insensitive to outliers. 
In general, think about the types of resistant measures (i.e., median or mean). 
This step is making sure to determine data patterns.

3. The natural scale/state that the data are at their best. 
This will be the step at which the scale of data can be helpful for analysis. 
The reexpressing data to a new scale by taking the square root or logarithmic scale.

4. The mostly known parts of EDA but is done in the way of accessing fit of the data. 
This is taught in every statistics 101 class. 
The growth of machine learning and prediction methods have now used residuals more in the toolbox to assessing the best prediction models.

```{r violin_plot, echo=FALSE,out.width='.49\\linewidth', fig.width=3, fig.height=3, fig.show='asis', fig.align='center'}
ggplot(ChickWeight, aes(x = Diet, y = weight)) + 
  geom_violin(aes(fill = Diet), trim = FALSE) +
  geom_boxplot(width = 0.1) +
  theme_classic() +
  theme(legend.position = "none")

p <- ggplot(ChickWeight, aes(x=Diet, y=weight, fill=Diet)) +
  geom_boxplot()
p + scale_fill_brewer(palette="Dark2")
```

Data visualizations are an integral part of the EDA process, enabling analysts to discern patterns and relationships in the data that would otherwise be difficult to discern from tabular data alone.
Through data visualization, analysts can quickly identify trends, outliers, and other patterns that may be missed through numerical analysis alone. 
Moreover, visualizations facilitate the communication of findings to non-technical stakeholders, allowing them to comprehend complex data sets more efficiently. 
Through visualizations, analysts can also identify potential issues or biases in the data, resulting in better decisions and models. 
Thus, visualizations play a crucial role in the EDA process by enabling analysts to more effectively explore, comprehend, and communicate data-derived insights.
During the initial EDA stage, an analyst may find that a variable or a covariate is directly related to the dependent variable when looking at a correlation heatmap or a scatterplot. 
The basic understanding can be formalized to visualize the discovery process.

The field of graphical communication, which is directly related to EDA, semiology, and their use in touch, has been a valuable tool and extension of the EDA thoughts that Tukey expressed.
One of the fundamental principles of semiology is the relationship between signifier and signified, in which a visual element (the signifier) represents a particular meaning or concept (the signified), [@barthes1972]. 
Another essential concept in semiology is using syntax and semantics to convey meaning in graphic communication effectively. 
This includes both the syntax and semantics of a graphic's visual elements, [@bertin1983].

Using color to represent data on maps is an example of successful graphical communication utilizing semiology. 
By using different colors to represent different data points, viewers can comprehend patterns and relationships in the data quickly and easily. 
Jacques Bertin  writes in "Semiology of Graphics" that color can be used to "emphasize a point, distinguish one category from another, or establish a relationship between two points", [@bertin1983].
In addition, Bertin explains that the use of color can help overcome language barriers, making it easier for the audience to comprehend the presented information.

The application of semiology in graphical communication is not devoid of obstacles. 
One difficulty is the possibility of misinterpretation, in which viewers may assign a different meaning to a visual element than was intended, [@bertin1983]. 
Another concern is the possibility of cultural differences in interpretation, in which a visual element may have a different meaning in one culture versus another, [@norman2013].

Exploratory Data Analysis (EDA) analyzes and summarizes a dataset to discover patterns, trends, and insights. 
It is a crucial step in the data analysis process and is often used to identify which variables are essential, what the data looks like, and what the underlying structure of the data is. 
EDA is typically done using various techniques, such as visualizations, statistical summaries, and data transformations.


## Open-Source 

Open-source data, deeply rooted in the foundational principles of shared knowledge that has driven human progress for millennia, has evolved significantly over the decades. 
Originating from the frustrations of figures like Richard Stallman in the 1980s, who championed the free software movement and paved the way for the creation of transformative licenses such as the GNU General Public License, the concept has grown to encompass more than just software. [@stallman2002]
With organizations like the Open Source Initiative standardizing open-source practices and monumental projects like Linux showcasing its potential, open-source principles expanded by the late 1990s to include data.
This movement towards openness has been marked by a commitment to transparency, collaboration, and unrestricted access, revolutionizing how we perceive and interact with data in the modern era.

The principles of open-source data revolve around the concepts of free access, transparency, collaboration, and redistribution, fostering a community-driven approach to data sharing and utilization.
At its core, open source is about making the source content (usually software code) freely available. 
This transparency allows anyone to review, inspect, and understand the source.
The freedom to use, modify, distribute, and study the source content without restrictions is a cornerstone of open source.
This is articulated in various open source licenses, like the GNU General Public License (GPL), which emphasizes the rights of end users.
Open source projects thrive on collaborative efforts. 
Diverse groups of people from around the world contribute, enhancing the project's robustness and creativity.
Tools like Git and platforms like GitHub have made this collaborative approach more streamlined.
Open source projects often foster strong communities. 
These communities not only contribute code but also offer support, documentation, and strategies for the project's future.
These communities' sense of belonging and shared purpose is a driving force behind many successful open-source projects.
Contributions to open source projects are often evaluated based on merit. 
The best ideas or implementations, regardless of their source, are adopted, promoting a culture of excellence.
The principle of redistribution ensures that modified versions of open source content remain open. 
This ensures a perpetual cycle of community-driven improvement and access.
Open source principles prioritize the end user’s interests. 
This user-centric approach often leads to software or content that's more aligned with what users genuinely need and want.


## Origins of Open Source

The concept of open source originated long before computers, where shared knowledge formed the basis of human progress.
With the advent of computers, Richard Stallman, frustrated with a printer that didn’t have available source code, initiated the free software movement in the 1980s and established the GNU Project and the Free Software Foundation [@stallman2002].
Linux, an open-source operating system kernel initiated by Linus Torvalds in 1991, became one of the most popular examples of open-source success.
The Apache HTTP Server, released in 1995, became another success story, powering a large fraction of the internet [@weber2004].
Open-source principles began expanding from software to data in the late 1990s and early 2000s. The idea was to share data sets for public use without restrictions.
Projects like the Human Genome Project advocated for open data to advance science and medicine [@sulston2002].

## Open Access Data Repositories

In the digital age, open access data repositories have become crucial platforms, transforming how data are shared, accessed, and stored among academic and research communities. 
These repositories are online spaces created specifically to store datasets and make them available to everyone. In line with the principles of open science, which promote knowledge sharing, transparency, and reproducibility in research endeavors, their main objective is to democratize access to data.

These repositories serve many different purposes at their core. 
They support openness in the scientific method first and foremost. 
It enables other researchers and the general public to examine, confirm, and replicate research findings by providing unrestricted access to datasets. Collaboration is also encouraged by this open model. 
Datasets are accessible to researchers in a variety of fields and locations, providing opportunities to expand upon, combine, or compare various sets of data. 
Eliminating research duplication is a significant additional benefit. 
Access to data from earlier studies reduces the need to repeatedly collect similar data, which saves time, effort, and resources. 
Additionally, these repositories are essential for the long-term preservation of data, guaranteeing that datasets are accessible for future scientific research and for posterity.

A closer look at the features of these repositories reveals several standard components. 
Metadata is paramount; this descriptive information elucidates the context, origin, methodology, and other pertinent details of the datasets, ensuring they are comprehensible to those accessing them. 
Many repositories also emphasize the citability of datasets. 
By assigning a Digital Object Identifier (DOI), datasets become easily referable in academic and research contexts. 
Licensing is another cornerstone. To clarify usage rights and conditions, datasets are often accompanied by explicit licenses, with Creative Commons licenses being particularly prevalent. 
These licenses can stipulate various conditions, with attribution being a common requirement. 
Furthermore, the user experience is enhanced through tools that facilitate easy searching, accessing, and downloading of datasets.

However, like all systems, open access data repositories face challenges. 
Ensuring data privacy is a significant concern, especially in datasets derived from human subjects, such as in health or social research. 
Standardization is another hurdle. 
Given the diverse range of researchers and datasets being uploaded, achieving a standardized format for data and metadata is daunting. 
And not to be overlooked is the challenge of sustainability. 
Maintaining a sophisticated digital platform requires both financial resources and technical expertise. 
Ensuring the longevity of such repositories, especially in a rapidly evolving digital landscape, remains a concern.

Several prominent examples underscore the significance and diversity of open access data repositories. 
Zenodo, developed under the European OpenAIRE program, serves as a multifaceted platform catering to researchers across disciplines. 
Dryad specializes in datasets linked to scientific publications, predominantly in life sciences and biomedicine. 
Figshare offers a broader spectrum, allowing researchers to deposit a range of research outputs, including datasets, making them available to the public.

Historically, the rise of these repositories can be contextualized within the broader open science movement. 
This movement, gathering momentum over recent decades, has been advocating for a more transparent, accessible, and shared research process. 
As technological advancements surged, leading to an exponential growth in digital data, the emphasis on preserving and sharing this data treasure trove intensified.

Today, the significance of open access data repositories is further underscored by policies and guidelines from influential bodies. 
A growing number of funding agencies and academic journals are either mandating or recommending that data underpinning research findings be housed in open access repositories. 
This trend not only ensures that research outputs, especially data, remain accessible but also reflects a broader societal push towards transparency, especially when research is funded by public coffers.


<!--     - Single Data Source with multiple variables (n > 10 columns) 
## Single Open-source Data

A single data source refers to a specific origin or repository from which data is derived or collected, ensuring uniformity and consistency in the information presented. 
It contrasts with multiple data sources where data is aggregated from various origins, potentially introducing discrepancies or inconsistencies. 
For example, a company might maintain a single database that logs all customer transactions. 
This database would be considered a single data source. 
Another illustration could be a researcher conducting a study on climate change, who decides to rely solely on temperature readings from a specific meteorological station; in this case, the meteorological station's data would be the single data source.
Utilizing a single data source simplifies data management and analysis but may also limit the available breadth and diversity of information. -->

<!--     - Multiple Data Sources with multiple variables (n > 10 columns) 

## Multiple Open-source Data

Multiple data sources refer to the amalgamation of data from various origins or repositories, providing a comprehensive and diverse piece of information. 
By drawing from different sources, analysts can gain a richer and more holistic view of the subject, although it may introduce data integration and consistency. 
For instance, in business intelligence, a company might combine sales data from their online store, physical retail outlets, and third-party distributors to get a complete picture of their sales performance. 
Similarly, a researcher studying urban air quality might merge satellite observations, ground-based air quality monitors, and citizen-reported data to achieve a multifaceted understanding. 
Leveraging multiple data sources offers depth and breadth in insights but necessitates careful management to ensure data integrity and comparability. -->


<!-- - Exploratory Data Analysis (EDA) -->

<!--     - Layout Framework of EDA with modern technology  -->


<!--     - R packages  -->
R has become a prominent and adaptable programming language within the domain of data analysis and exploratory data analysis (EDA). 
A number of crucial R packages have been created to streamline the processes of data manipulation, visualization, and statistical analysis. 
One of the prominent programs in this category is ggplot2, which was developed by Hadley Wickham. 
This package is widely recognized for its versatility and aesthetic appeal in generating a diverse range of visually appealing and informative data visualizations [@wickham2016]. 
Wickham et al. have developed dplyr and tidyr, which offer convenient functionalities for filtering, summarizing, and reshaping data frames, hence enhancing the efficiency of data manipulation activities [@wickham2021]. 
The data.table package, developed by Matt Dowle, is widely recognized for its remarkable speed and efficiency in handling huge datasets, particularly in the context of performance-oriented data manipulation tasks [@dowle2021]. 
Although not classified as a R package, it is noteworthy to include Wes McKinney's Python library, pandas, due to its significant influence in the field of data manipulation and analysis in the Python programming language [@mcKinney2010]. 
Within the field of psychology and psychometrics, the psych package developed by William Revelle serves as an extensive tool for doing factor analysis and reliability analysis [@revelle202]. 

Furthermore, there are other software packages available that may be utilized by researchers and analysts to enhance their data management and analysis processes. 
For instance, the naniar package, developed by [@tierney2020], is specifically designed for handling missing data. Another useful package is summarytools, created by [@comtois2021], which enables the generation of descriptive statistics. 
Additionally, the corrplot package, developed by Wei and Simko [@wei2017], provides significant tools for displaying correlation matrices. 
These packages provide researchers and analysts a range of valuable resources to aid in their work. 
The FactoMineR package, developed by Husson, Lê, and Pagès [@husson2020], offers crucial techniques for conducting multivariate data analysis, including principal component analysis and clustering. 
These software programs, created by renowned individuals in the discipline, jointly provide data analysts and scientists with the necessary tools to efficiently investigate and analyze data.


Here are some popular R packages for exploratory data analysis along with a brief summary:

1. **ggplot2**:
   - Summary: ggplot2 is a powerful data visualization package for creating a wide range of high-quality plots and charts. 
   It follows the Grammar of Graphics framework, making it highly customizable and flexible.
   - Author: Hadley Wickham

2. **dplyr**:
   - Summary: dplyr is a package for data manipulation. 
   It provides a set of easy-to-use functions that facilitate common data manipulation tasks such as filtering, selecting, grouping, and summarizing data.
   - Author: Hadley Wickham

3. **tidyr**:
   - Summary: tidyr is another package by Hadley Wickham that is used for tidying and reshaping data. 
   It helps in converting data from wide to long format and vice versa, making it suitable for analysis and visualization.

4. **data.table**:
   - Summary: data.table is an efficient package for data manipulation and analysis. 
   It is particularly well-suited for large datasets and complex operations.

5. **pandas** (not an R package, but widely used in Python for EDA):
   - Summary: pandas is a Python library for data manipulation and analysis. 
   It provides data structures like DataFrames and Series, along with a wide range of functions for data cleaning and analysis.

6. **psych**:
   - Summary: The psych package in R provides various functions for conducting psychometric and psychological research. 
   It includes tools for factor analysis, reliability analysis, and data visualization.

7. **naniar**:
   - Summary: naniar is an R package that helps with missing data exploration and visualization. 
   It provides functions for identifying, visualizing, and handling missing data in datasets.

8. **summarytools**:
   - Summary: summarytools is a package for creating beautiful and informative summary tables and charts for descriptive statistics. 
   It can be helpful for summarizing and exploring data.

9. **corrplot**:
   - Summary: corrplot is a package for visualizing correlation matrices. 
   It provides various plot types and customization options for exploring relationships between variables.

10. **FactoMineR**:
    - Summary: FactoMineR is a package for multivariate data analysis, including methods for principal component analysis (PCA), factor analysis, and clustering. 
    It is useful for exploring complex datasets.

<!--     - The importance of the grammar of data manipulation (dplyr and tidyverse)  -->


<!-- - What is Data Manipulation? -->
<!--     - A formal definition and construction of data manipulation  -->
<!--     - The application of data manipulation  -->
<!-- - Summary -->




